{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b748cd",
   "metadata": {},
   "source": [
    "**Disclaimer**: This educational content, including any code examples, is provided for instructional purposes only. The author does not endorse or encourage the unauthorised or illegal scraping of websites.\n",
    "\n",
    "While Python with releveant libraries can be used for web scraping, it's crucial to conduct scraping activities in compliance with applicable laws, the website's terms of service, and ethical considerations. Always review and respect the rules set by the website you are scraping to ensure legal and responsible data collection practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6196b69",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**: Exercie 2 will only work on your own machine (not via noteable, I think). Also: You'll need to unzip the chrome driver in the folder where your notebook is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889b428",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6bea1",
   "metadata": {},
   "source": [
    "Just using beautiful soup go to the description of this course and extract the number of SCQF level for it. http://www.drps.ed.ac.uk/22-23/dpt/cxcmse11427.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da2879",
   "metadata": {},
   "source": [
    "Spoiler alert: the content should be 'SCQF Level ...'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff05fc",
   "metadata": {},
   "source": [
    "*hint*: you can use in to check for occurance of a word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30392d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T21:54:51.536037Z",
     "start_time": "2024-01-15T21:54:51.531052Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'banana' in 'a sentence with bananas':\n",
    "    print('this sentence has a word banana in it')\n",
    "\n",
    "# if word in sentence:\n",
    "#     print('this sentence has a word in it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f924c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T21:54:55.807581Z",
     "start_time": "2024-01-15T21:54:55.803799Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install bs4 \n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b9512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T21:54:57.389986Z",
     "start_time": "2024-01-15T21:54:57.339151Z"
    }
   },
   "outputs": [],
   "source": [
    "# website address\n",
    "page = 'http://www.drps.ed.ac.uk/22-23/dpt/cxcmse11427.htm'\n",
    "\n",
    "# open the url and store the website\n",
    "website = urlopen(page)\n",
    "\n",
    "# convert the website's content, for this a parser is needed. In this case a html parser\n",
    "soup = BeautifulSoup(website, 'html.parser')\n",
    "\n",
    "# Retrieve the cell that contains 'SCQF Level'\n",
    "# Retrieve the cell that contains 'SCQF Credits'\n",
    "table = soup.find('table', {'class':'sitstablegrid'})\n",
    "for cell in table.findChildren('td'):\n",
    "    if 'SCQF Level' in cell.text:\n",
    "        print(cell.text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56066ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43b1ad17",
   "metadata": {},
   "source": [
    "# Exercise 2 (ONLY in your own machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ba140",
   "metadata": {},
   "source": [
    "Again, we hope to find all reviews of Edinburgh castle, but now we don't have API key. \n",
    "\n",
    "**It is NOT suggested for any commerical use**, but for educational purposes, you can have a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0ae20",
   "metadata": {},
   "source": [
    "We look at the Google map, Edinburgh castle: https://www.google.com/maps/place/Edinburgh+Castle/@55.9485977,-3.2021022,17z/data=!4m5!3m4!1s0x4887c79a2099c0f7:0x469a1eebe54c0a58!8m2!3d55.9485947!4d-3.1999135"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2e78c",
   "metadata": {},
   "source": [
    "And we try to find all reviews of Edinburgh castle, OR we can try to get all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae783ded",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T18:40:31.742093Z",
     "start_time": "2024-01-15T18:40:31.737098Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install selenium \n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf61fdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T17:44:14.137393Z",
     "start_time": "2024-01-15T17:44:14.132635Z"
    }
   },
   "outputs": [],
   "source": [
    "# define method that will create a browser, suitable to your operating system\n",
    "import sys\n",
    "def get_a_browser():\n",
    "    if sys.platform.startswith('win32') or sys.platform.startswith('cygwin'):\n",
    "        return webdriver.Chrome() # windows\n",
    "    else:\n",
    "        return webdriver.Chrome('./chromedriver') # mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c651f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T17:50:00.634402Z",
     "start_time": "2024-01-15T17:50:00.630180Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is Edinburgh castle\n",
    "url = 'https://www.google.com/maps/place/Edinburgh+Castle/@55.9485947,-3.2021022,17z/data=!3m1!4b1!4m5!3m4!1s0x4887c79a2099c0f7:0x469a1eebe54c0a58!8m2!3d55.9485947!4d-3.1999135'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2d7b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:03:31.456682Z",
     "start_time": "2024-01-15T19:03:19.799792Z"
    }
   },
   "outputs": [],
   "source": [
    "browser = get_a_browser()\n",
    "\n",
    "# the url we want to open\n",
    "# fill in the correct url for google map, Edinburgh castle!\n",
    "# You can also try other places!\n",
    "\n",
    "# the browser will start and load the webpage\n",
    "browser.get(url)\n",
    "\n",
    "# we wait 1 second to let the page load everything\n",
    "time.sleep(1)\n",
    "\n",
    "# Google map has cookies warning, just accept the cookies\n",
    "# In the previous, we use a div, span, h1 to find the elements, however, if you need to find a specific element, \n",
    "# then xpath is a better option\n",
    "xpath_for_accept_cookies = '//*[@id=\"yDmH0d\"]/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[2]/div/div/button'\n",
    "accept_cookies = browser.find_element(By.XPATH, xpath_for_accept_cookies)\n",
    "print(accept_cookies)\n",
    "accept_cookies.click();\n",
    "time.sleep(3)\n",
    "\n",
    "# Get total number of reviews\n",
    "xpath_for_total_number_of_reviews = '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div/div[1]/div[2]/div/div[1]/div[2]/span[2]/span/span'\n",
    "total_number_of_reviews = browser.find_element(By.XPATH, xpath_for_total_number_of_reviews)\n",
    "# The number we get is '(89,862) reviews', but we hope to get 89862!\n",
    "total_number_of_reviews = int(total_number_of_reviews.text.split(\" \")[0].replace('(', '').replace(')', '').replace(',', ''))\n",
    "print('There are', total_number_of_reviews, 'reviews')\n",
    "\n",
    "# go to all reviews page\n",
    "xpath_for_all_reviews ='//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]'\n",
    "browser.find_element(By.XPATH, xpath_for_all_reviews).click()\n",
    "# some other xpath may work, you can have a try!\n",
    "time.sleep(3)\n",
    "\n",
    "print('Pleasae KEEP BROWSER OPEN!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll down to access all reviews, first of all, let's find the element!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ef043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:07:01.940721Z",
     "start_time": "2024-01-15T19:06:50.999937Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There are many ways to scroll down\n",
    "\"\"\"\n",
    "### 1. Use xpath\n",
    "xpath_for_scrollable_div = '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]'\n",
    "scrollable_div = browser.find_element(By.XPATH, xpath_for_scrollable_div)\n",
    "\n",
    "### 2. use css selector\n",
    "# css_for_scrollable_div = '#QA0Szd > div > div > div.w6VYqd > div:nth-child(2) > div > div.e07Vkf.kA9KIf'\n",
    "# scrollable_div = browser.find_element(By.CSS_SELECTOR, css_for_scrollable_div)\n",
    "\n",
    "#Scroll as many times as necessary to load all reviews\n",
    "\"\"\"\n",
    "Google map reviews display 10 per scoll down, so in total, we need to run round(total_number_of_reviews/10 - 1) times;\n",
    "However Google has a limit maximum of requests 120.\n",
    "\"\"\"\n",
    "# for i in range(0,(round(total_number_of_reviews/10 - 1))):\n",
    "for i in range(0,10):\n",
    "    browser.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', \n",
    "            scrollable_div)\n",
    "    time.sleep(1)\n",
    "    \n",
    "### 3. PAGE DOWN method\n",
    "# Find the body of the page\n",
    "# body = browser.find_element(By.TAG_NAME,'body')\n",
    "# # Keep scrolling down using a simulation of the PAGE_DOWN button\n",
    "# for _ in range(10):\n",
    "#     body.send_keys(Keys.PAGE_DOWN)\n",
    "#     time.sleep(1)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "#extract the info\n",
    "response = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "reviews = response.find_all('span', class_='wiI7pd', string=True)\n",
    "dates = response.find_all('span', class_='rsqaWe', string=True)\n",
    "rates = response.find_all('span', class_=\"kvMYJc\")\n",
    "users = response.find_all('div', class_='jftiEf fontBodyMedium')\n",
    "\n",
    "reviews_text=[]\n",
    "dates_text = []\n",
    "users_text = []\n",
    "rates_int = []\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    reviews_text.append(reviews[i].text)\n",
    "    dates_text.append(dates[i].text)\n",
    "    rates_int.append(int(rates[i]['aria-label'][0]))\n",
    "    users_text.append(users[i]['aria-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed8065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:05:09.678643Z",
     "start_time": "2024-01-15T19:05:09.673205Z"
    }
   },
   "outputs": [],
   "source": [
    "#Build up a pandas dataframe to store the dataset\n",
    "df = pd.DataFrame({\"users\":users_text, \n",
    "                   \"rates\":rates_int, \n",
    "                   \"review\":reviews_text, \n",
    "                   \"dates\":dates_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b366d30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:05:10.897402Z",
     "start_time": "2024-01-15T19:05:10.887038Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10e981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c08bc442",
   "metadata": {},
   "source": [
    "#  Exercise 3: Get Property info from ESPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4924313",
   "metadata": {},
   "source": [
    "About ESPC:\n",
    "\n",
    "ESPC is the home of property, and the first-choice property portal for home buyers and sellers in Edinburgh, the Lothians, Fife and the Borders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1b8e73",
   "metadata": {},
   "source": [
    "* Find recent property listings\n",
    "* Write the necessary info to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474953f",
   "metadata": {},
   "source": [
    "**AGAIN, the approach is valuable, however scraping website may have legal issues!**\n",
    "\n",
    "**Please use official API to get the dataset from the websites if possible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc04ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T22:02:49.643590Z",
     "start_time": "2024-01-15T22:02:49.640669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a5c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T22:07:19.921364Z",
     "start_time": "2024-01-15T22:07:19.865405Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7defb",
   "metadata": {},
   "source": [
    "## Here are the code to scrape the property information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4947f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T22:07:21.055383Z",
     "start_time": "2024-01-15T22:07:21.034854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the EspcScraper class\n",
    "class EspcScraper:\n",
    "    # Initializer or constructor method\n",
    "    def __init__(self):\n",
    "        # Initialize a list to store the results\n",
    "        self.results = []\n",
    "    \n",
    "    # Method to fetch the HTML content of a given URL\n",
    "    def fetch(self, url):\n",
    "        print('HTTP GET request to URL: %s' % url, end='')\n",
    "        res = requests.get(url)  # Perform the HTTP GET request\n",
    "        print(' | Status code: %s' % res.status_code)\n",
    "        return res\n",
    "\n",
    "    # Method to parse the HTML content and extract relevant data\n",
    "    def parse(self, html):\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        content = BeautifulSoup(html, 'lxml')\n",
    "        # Find all div elements with the 'infoWrap' class\n",
    "        # This is because the web page is structured in such a way that, card by card\n",
    "        cards = content.findAll('div', {'class': 'infoWrap'})\n",
    "        \n",
    "        # Define delimiters for splitting the title\n",
    "        delimiters = \":|,\"\n",
    "        \n",
    "        # Iterate over each card and extract data\n",
    "        for card in cards:\n",
    "            # # Extract the hyperlink\n",
    "            link = 'https://espc.com' + card.find('a')['href']\n",
    "            \n",
    "            # title includes property type, address, town and postcode\n",
    "            title = card.find('h3', {'class': 'propertyTitle'}).text\n",
    "            title_part = re.split(delimiters, title)\n",
    "            postcode = title_part[-1].strip()\n",
    "            if postcode.count(' ') > 1:\n",
    "                town = postcode.split()[0]\n",
    "                postcode = ' '.join(postcode.split()[-2:])\n",
    "            else:\n",
    "                town = title_part[2].strip()\n",
    "\n",
    "            # description\n",
    "            description = card.find('div', {'class': 'description'})\n",
    "            \n",
    "            # price & offer\n",
    "            offer_type = card.find('span', {'class': 'offersOver'}).text\n",
    "            price = card.find('span', {'class': 'price'}).text\n",
    "            \n",
    "            # facilities\n",
    "            facilities = card.find('div', {'class': 'facilities'}).text\n",
    "            # Ensure the list is at least 3 elements long by extending with 'U'\n",
    "            facilities = (facilities + 'U' * 3)[:3]\n",
    "\n",
    "            try:\n",
    "                agent = card.find('div', {'class': 'logoWrap'}).find('img')['alt']\n",
    "            except:\n",
    "                agent = 'N/A'\n",
    "            \n",
    "            # Append the extracted information to the results list\n",
    "            self.results.append({\n",
    "                'offer_type': offer_type[:-len(price) - 1],\n",
    "                'price': price.strip(),\n",
    "                'property_type': title_part[0].strip(),\n",
    "                'address': title_part[1].strip(),\n",
    "                'town': town,\n",
    "                'postcode': postcode,\n",
    "                'area': postcode.split()[0],\n",
    "                'beds': facilities[0],\n",
    "                'toilets': facilities[1],\n",
    "                'living_rooms': facilities[2],\n",
    "                'description': description.text.split('\\n', 1)[0],\n",
    "                'link': link,\n",
    "                'parking': 'parking' in description,\n",
    "                'allocated': 'allocated' in description,\n",
    "                'agent': agent\n",
    "            })\n",
    "\n",
    "    def run(self,url, result_saving=False):\n",
    "        res = self.fetch(url)\n",
    "        self.parse(res.text)\n",
    "        \n",
    "        # get number of pages\n",
    "        li_tags = BeautifulSoup(res.text, 'lxml').select('ul.paginationList > li')\n",
    "        n_pages = int(li_tags[-2].text)\n",
    "        \n",
    "        # get all pages\n",
    "        insert_position = url.find('?') + 1  # +1 to insert after the '?'\n",
    "        for page in range(2, n_pages + 1):\n",
    "            string_to_insert = 'p=' + str(page) + '&'\n",
    "            new_url = url[:insert_position] + string_to_insert + url[insert_position:]\n",
    "            new_res = self.fetch(new_url)\n",
    "            self.parse(new_res.text)\n",
    "            time.sleep(2)\n",
    "            \n",
    "        df = pd.DataFrame(self.results)\n",
    "        # Just clean the empty rows\n",
    "        df.drop_duplicates(subset=['agent',\n",
    "                                   'address',\n",
    "                                   'price'],\n",
    "                           inplace=True)\n",
    "        \n",
    "        # If result_saving is True, then save results\n",
    "        if result_saving:\n",
    "            print('Saving ', len(df), ' items to csv file (espc.csv)...')\n",
    "            df.to_csv('espc.csv', index=False,\n",
    "                      encoding='utf-8-sig')\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef658dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T22:08:56.100923Z",
     "start_time": "2024-01-15T22:07:27.371075Z"
    }
   },
   "outputs": [],
   "source": [
    "# For example, \n",
    "url = 'https://espc.com/properties?locations=edinburgh&minbeds=2plus&maxprice=300000'\n",
    "scraper = EspcScraper()\n",
    "scraper.run(url, result_saving=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427a66c",
   "metadata": {},
   "source": [
    "You can also try to get all other properties you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323addd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee7295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acef257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
