{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882babf6",
   "metadata": {},
   "source": [
    "**Disclaimer**: This educational content, including any code examples, is provided for instructional purposes only. The author does not endorse or encourage the unauthorised or illegal scraping of websites.\n",
    "\n",
    "While Python with releveant libraries can be used for web scraping, it's crucial to conduct scraping activities in compliance with applicable laws, the website's terms of service, and ethical considerations. Always review and respect the rules set by the website you are scraping to ensure legal and responsible data collection practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fb5c0",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**: Exercie 2 will ONLY work on your own machine (not via noteable, I think). Also: You'll need to unzip the chrome driver in the folder where your notebook is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b859dfe",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87d2cc",
   "metadata": {},
   "source": [
    "Just using beautiful soup go to the description of this course and extract the number of SCQF level for it. http://www.drps.ed.ac.uk/22-23/dpt/cxcmse11427.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403332df",
   "metadata": {},
   "source": [
    "Spoiler alert: the content should be 'SCQF Level ...'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2911ab",
   "metadata": {},
   "source": [
    "*hint*: you can use in to check for occurance of a word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce41b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:22:41.768973Z",
     "start_time": "2023-01-15T22:22:41.754967Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'banana' in 'a sentence with bananas':\n",
    "    print('this sentence has a word banana in it')\n",
    "\n",
    "# if word in sentence:\n",
    "#     print('this sentence has a word in it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32647c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:22:43.374468Z",
     "start_time": "2023-01-15T22:22:43.096736Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e282009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:22:44.649613Z",
     "start_time": "2023-01-15T22:22:44.493831Z"
    }
   },
   "outputs": [],
   "source": [
    "# website address\n",
    "page = 'http://www.drps.ed.ac.uk/22-23/dpt/cxcmse11427.htm'\n",
    "\n",
    "# open the url and store the website\n",
    "website = urlopen(page)\n",
    "\n",
    "# convert the website's content, for this a parser is needed. In this case a html parser\n",
    "soup = BeautifulSoup(website, 'html.parser')\n",
    "\n",
    "# Retrieve the cell that contains 'SCQF Level'\n",
    "# ....\n",
    "\n",
    "# hints:\n",
    "#     what type of a tag is it in? how can you get all items of that type? \n",
    "#     how will you check if they incude these words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac11188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0d8b631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:20:09.653625Z",
     "start_time": "2023-01-15T22:20:09.644635Z"
    }
   },
   "source": [
    "# Exercise 2: Get Review from Google Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711600fb",
   "metadata": {},
   "source": [
    "Again, we hope to find all reviews of Edinburgh castle, but now we don't have API key. \n",
    "\n",
    "**It is NOT suggested for any commerical use**, but for educational purposes, you can have a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d8fd3",
   "metadata": {},
   "source": [
    "We look at the Google map, Edinburgh castle: https://www.google.com/maps/place/Edinburgh+Castle/@55.9485977,-3.2021022,17z/data=!4m5!3m4!1s0x4887c79a2099c0f7:0x469a1eebe54c0a58!8m2!3d55.9485947!4d-3.1999135"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8fd8d",
   "metadata": {},
   "source": [
    "And we try to find all reviews of Edinburgh castle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae66bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:23:09.035397Z",
     "start_time": "2023-01-15T22:23:09.030400Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install selenium \n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050531a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:23:11.670964Z",
     "start_time": "2023-01-15T22:23:11.657972Z"
    }
   },
   "outputs": [],
   "source": [
    "# define method that will create a browser, suitable to your operating system\n",
    "import sys\n",
    "def get_a_browser():\n",
    "    if sys.platform.startswith('win32') or sys.platform.startswith('cygwin'):\n",
    "        return webdriver.Chrome() # windows\n",
    "    else:\n",
    "        return webdriver.Chrome('./chromedriver') # mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '.....'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b72e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:31:23.918586Z",
     "start_time": "2023-01-15T22:31:08.183430Z"
    }
   },
   "outputs": [],
   "source": [
    "browser = get_a_browser()\n",
    "\n",
    "# the url we want to open\n",
    "# fill in the correct url for google map, Edinburgh castle!\n",
    "# You can also try other places\n",
    "# Open Google Maps, and search, \"Edinburgh Castle\", Copy the url and paste it here\n",
    "\n",
    "browser.get(url)\n",
    "# the browser will start and load the webpage\n",
    "\n",
    "# we wait 1 second to let the page load everything\n",
    "time.sleep(1)\n",
    "\n",
    "# Google map has cookies warning, just accept the cookies\n",
    "\n",
    "# Find XPATH of cookies button, \n",
    "xpath_for_accept_cookies = '...'\n",
    "accept_cookies = browser.find_element(By.XPATH, xpath_for_accept_cookies)\n",
    "print(accept_cookies)\n",
    "accept_cookies.click();\n",
    "time.sleep(3)\n",
    "\n",
    "# Get total number of reviews\n",
    "# Find Xpath of total number of reviews\n",
    "total_number_of_reviews = browser.find_element(By.XPATH, '...').text.split(\" \")[0]\n",
    "total_number_of_reviews = int(total_number_of_reviews.text.split(\" \")[0].replace('(', '').replace(')', '').replace(',', ''))\n",
    "print('There are', total_number_of_reviews, 'reviews')\n",
    "\n",
    "# go to all reviews page\n",
    "# Find XPATH of all review link\n",
    "all_reviews_page = browser.find_element(By.XPATH, '...').click()\n",
    "# some other xpath may work, you can have a try!\n",
    "time.sleep(3)\n",
    "\n",
    "print('Pleasae KEEP BROWSER OPEN!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ecd06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:31:49.715169Z",
     "start_time": "2023-01-15T22:31:38.445986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scroll down to access all reviews, first of all, let's find the element!\n",
    "# Find XPATH of Scroll button!\n",
    "scrollable_div = browser.find_element(By.XPATH,\n",
    "                                      '...')#Scroll as many times as necessary to load all reviews\n",
    "\"\"\"\n",
    "Google map reviews display 10 per scoll down, so in total, we need to run round(total_number_of_reviews/10 - 1) times\n",
    "However Google has a limit maximum of requests 120, therefore, if the number of reviews are too many, we can just get the part of them\n",
    "\"\"\"\n",
    "# for i in range(0,(round(total_number_of_reviews/10 - 1))):\n",
    "# # However, in order to run fast, we can just load part of all\n",
    "for i in range(0,10):\n",
    "    browser.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', \n",
    "            scrollable_div)\n",
    "    time.sleep(1)\n",
    "\n",
    "#extract the info\n",
    "response = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "#Find the relevant class names!\n",
    "reviews = response.find_all('span', class_='...', text=True)\n",
    "dates = response.find_all('span', class_='...', text=True)\n",
    "rates = response.find_all('span', class_=\"...\")\n",
    "users = response.find_all('div', class_='...')\n",
    "\n",
    "reviews_text=[]\n",
    "dates_text = []\n",
    "users_text = []\n",
    "rates_int = []\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    reviews_text.append(reviews[i].text)\n",
    "    dates_text.append(dates[i].text)\n",
    "    rates_int.append(int(rates[i]['aria-label'][1]))\n",
    "    users_text.append(users[i]['aria-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b16cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:31:55.421919Z",
     "start_time": "2023-01-15T22:31:55.403930Z"
    }
   },
   "outputs": [],
   "source": [
    "#Build up a pandas dataframe to store the dataset\n",
    "df = pd.DataFrame({\"users\":users_text, \n",
    "                   \"rates\":rates_int, \n",
    "                   \"review\":reviews_text, \n",
    "                   \"dates\":dates_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c12350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T22:31:59.315324Z",
     "start_time": "2023-01-15T22:31:59.283347Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5078cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6b77920",
   "metadata": {},
   "source": [
    "# Exercise 3: Get Property info from ESPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b9410",
   "metadata": {},
   "source": [
    "About ESPC:\n",
    "\n",
    "ESPC is the home of property, and the first-choice property portal for home buyers and sellers in Edinburgh, the Lothians, Fife and the Borders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecd456",
   "metadata": {},
   "source": [
    "* Find recent property listings\n",
    "* Write the necessary info to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986256ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63288aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T21:25:51.631752Z",
     "start_time": "2024-01-15T21:25:51.627730Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda2e0f",
   "metadata": {},
   "source": [
    "## Here are the code to scrape the property information, please fill the blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf687b26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T21:50:51.308677Z",
     "start_time": "2024-01-15T21:50:51.288810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the EspcScraper class\n",
    "class EspcScraper:\n",
    "    # Initializer or constructor method\n",
    "    def __init__(self):\n",
    "        # Initialize a list to store the results\n",
    "        self.results = []\n",
    "    \n",
    "    # Method to fetch the HTML content of a given URL\n",
    "    def fetch(self, url):\n",
    "        print('HTTP GET request to URL: %s' % url, end='')\n",
    "        res = requests.get(url)  # Perform the HTTP GET request\n",
    "        print(' | Status code: %s' % res.status_code)\n",
    "        return res\n",
    "\n",
    "    # Method to parse the HTML content and extract relevant data\n",
    "    def parse(self, html):\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        content = BeautifulSoup(html, 'lxml')\n",
    "        # Find all div elements with the 'infoWrap' class\n",
    "        # This is because the web page is structured in such a way that, card by card\n",
    "        cards = content.findAll('div', {'class': '...'})\n",
    "        \n",
    "        # Define delimiters for splitting the title\n",
    "        delimiters = \":|,\"\n",
    "        \n",
    "        # Iterate over each card and extract data\n",
    "        for card in cards:\n",
    "            # # Extract the hyperlink\n",
    "            link = 'https://espc.com' + card.find('a')['href']\n",
    "            \n",
    "            # title includes property type, address, town and postcode\n",
    "            title = card.find('h3', {'class': '...'}).text\n",
    "            title_part = re.split(delimiters, title)\n",
    "            postcode = title_part[-1].strip()\n",
    "            if postcode.count(' ') > 1:\n",
    "                town = postcode.split()[0]\n",
    "                postcode = ' '.join(postcode.split()[-2:])\n",
    "            else:\n",
    "                town = title_part[2].strip()\n",
    "\n",
    "            # description\n",
    "            description = card.find('div', {'class': '...'})\n",
    "            \n",
    "            # price & offer\n",
    "            offer_type = card.find('span', {'class': '...'}).text\n",
    "            price = card.find('span', {'class': '...'}).text\n",
    "            \n",
    "            # facilities\n",
    "            facilities = card.find('div', {'class': '...'}).text\n",
    "            # Ensure the list is at least 3 elements long by extending with 'U'\n",
    "            facilities = (facilities + 'U' * 3)[:3]\n",
    "\n",
    "            try:\n",
    "                agent = card.find('div', {'class': '...'}).find('img')['alt']\n",
    "            except:\n",
    "                agent = 'N/A'\n",
    "            \n",
    "            # Append the extracted information to the results list\n",
    "            self.results.append({\n",
    "                # ... (include all extracted data here)\n",
    "            })\n",
    "\n",
    "    def run(self,url, result_saving=False):\n",
    "        res = self.fetch(url)\n",
    "        self.parse(res.text)\n",
    "        \n",
    "        # get number of pages\n",
    "        li_tags = BeautifulSoup(res.text, 'lxml').select('ul.paginationList > li')\n",
    "        n_pages = int(li_tags[-2].text)\n",
    "        \n",
    "        # get all pages\n",
    "        insert_position = url.find('?') + 1  # +1 to insert after the '?'\n",
    "        for page in range(2, n_pages + 1):\n",
    "            string_to_insert = 'p=' + str(page) + '&'\n",
    "            new_url = url[:insert_position] + string_to_insert + url[insert_position:]\n",
    "            new_res = self.fetch(new_url)\n",
    "            self.parse(new_res.text)\n",
    "            time.sleep(2)\n",
    "            \n",
    "        df = pd.DataFrame(self.results)\n",
    "        # Just clean the empty rows\n",
    "        df.drop_duplicates(subset=['agent',\n",
    "                                   'address',\n",
    "                                   'price'],\n",
    "                           inplace=True)\n",
    "        \n",
    "        # If result_saving is True, then save results\n",
    "        if result_saving:\n",
    "            print('Saving ', len(df), ' items to csv file (espc.csv)...')\n",
    "            df.to_csv('espc.csv', index=False,\n",
    "                      encoding='utf-8-sig')\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683de5db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T22:05:00.787451Z",
     "start_time": "2024-01-15T22:05:00.783988Z"
    }
   },
   "outputs": [],
   "source": [
    "# for example set the url\n",
    "url = 'https://espc.com/properties?locations=edinburgh&minbeds=2plus&maxprice=300000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a84c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T21:52:20.127292Z",
     "start_time": "2024-01-15T21:50:56.440740Z"
    }
   },
   "outputs": [],
   "source": [
    "scraper = EspcScraper()\n",
    "scraper.run(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4a052",
   "metadata": {},
   "source": [
    "You can also try to get all other properties you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d2a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aee83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ebd89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
