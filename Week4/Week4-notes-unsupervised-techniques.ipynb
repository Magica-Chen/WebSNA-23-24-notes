{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e90cad",
   "metadata": {},
   "source": [
    "In this notebook, we will look at two parts, **clustering** and **recommender system**.\n",
    "\n",
    "In the first part, we will go through `KMeans` and `DBSCAN` algorithm to cluster `Starbucks` location geographically. We will use `scikit-learn` for implementation and `matplotlib` for visualisation.\n",
    "\n",
    "In the second part, we will consider recommender systems with decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d446cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79571c2",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9c2fa",
   "metadata": {},
   "source": [
    "First, let's look at the data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ed74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv with pandas, index indicates whether the first column is an index\n",
    "data = pd.read_csv(\"data/starbucks_locations.csv\", index_col=0)\n",
    "# We don't need NAs, so drop them!\n",
    "data = data.dropna()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ef029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data for computability. just keep first 200\n",
    "# data = data[:200]\n",
    "\n",
    "# or reduce data to just some part of the planet\n",
    "new_data = data[(data[\"Longitude\"].between(49, 51)) & (data[\"Latitude\"].between(26, 27))]\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766046f",
   "metadata": {},
   "source": [
    "Now we can use `scikit-learn` to implement both `k-means` and `DBSCAN`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1fa82",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8295f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means with 10 neighbours and 500 iterations\n",
    "kmeans = KMeans(n_clusters=10, \n",
    "                max_iter=500)\n",
    "\n",
    "print('Start K-means: ')\n",
    "kmeans.fit(data)\n",
    "print(\"Cluster centroids: \" + str(kmeans.cluster_centers_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cca91",
   "metadata": {},
   "source": [
    "To visualise the results, we need to write a plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphClusters(all_data, labels, centers):\n",
    "    # open a color map (cmap) which is a collection of colors which you can fetch by index (i.e. an integer)\n",
    "    cmap = plt.colormaps.get_cmap('tab10')\n",
    "    \n",
    "    # add the color as a new column in the dataframe and add the cluster labels\n",
    "    all_data['color'] = labels\n",
    "    \n",
    "    # add the centroids (in case of K-means) to the plot by adding their coordinates  \n",
    "    # example pyplot use: plt.plot(x, y, 'bo', linestyle='dashed')\n",
    "    # example pyplot use: plt.plot(x, y,  color='blue', marker='o', linestyle='dashed')\n",
    "    # more here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#main-content\n",
    "    \n",
    "    which_centroid = 0\n",
    "    if centers:\n",
    "        for center in centers:     \n",
    "            plt.plot(center[0],center[1], marker='o',label='centroid_'+str(which_centroid),\n",
    "                     markersize=10,color=cmap(which_centroid))\n",
    "            which_centroid += 1\n",
    "    \n",
    "    # add all the data points and use their label number to fetch a certain color\n",
    "    # index is    \n",
    "    for ind,row in all_data.iterrows():\n",
    "        plt.plot(row['Longitude'],row['Latitude'], marker='o', color=cmap(int(row['color'])), markersize=2)\n",
    "        \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The visulisation requires a long time, please be patient. Or you can try new_data or any other subset of original dataset\n",
    "# For example, using `new_data` to replace `data`\n",
    "graphClusters(data,kmeans.labels_,list(kmeans.cluster_centers_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc4ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b94179f",
   "metadata": {},
   "source": [
    "## Open questions: why is it 10 clusters? How about less or more? Why iteration = 500? Is it possible to have less iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec62363",
   "metadata": {},
   "source": [
    "The best tutorial of a Python library is always the official documentation, for example, `K-means`: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f7abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46d20c2d",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cab647",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=10,\n",
    "                min_samples=80,\n",
    "                metric='euclidean')\n",
    "\n",
    "print('Start DBSCAN: ')\n",
    "dbscan.fit(data)\n",
    "print(\"Labels: \"+str(set(dbscan.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecc5e1",
   "metadata": {},
   "source": [
    "As we discussed in the lecture, DBSCAN will finally find the outliers. In this case, `-1` means the outlier. So we can revise the plot function to show the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c62f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphClusters_DBSCAN(all_data, labels, centers):\n",
    "    # open a color map (cmap) which is a collection of colors which you can fetch by index (i.e. an integer)\n",
    "    cmap = plt.colormaps.get_cmap('tab10')\n",
    "    \n",
    "    # add the color as a new column in the dataframe and add the cluster labels\n",
    "    all_data['color'] = labels\n",
    "    \n",
    "    # add the centroids (in case of K-means) to the plot by adding their coordinates  \n",
    "    # example pyplot use: plt.plot(x, y, 'bo', linestyle='dashed')\n",
    "    # example pyplot use: plt.plot(x, y,  color='blue', marker='o', linestyle='dashed')\n",
    "    # more here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#main-content\n",
    "    \n",
    "    which_centroid = 0\n",
    "    if centers:\n",
    "        for center in centers:     \n",
    "            plt.plot(center[0],center[1], marker='o',label='centroid_'+str(which_centroid),\n",
    "                     markersize=10,color=cmap(which_centroid))\n",
    "            which_centroid += 1\n",
    "    \n",
    "    # add all the data points and use their label number to fetch a certain color\n",
    "    # index is    \n",
    "    for ind,row in all_data.iterrows():\n",
    "        if row['color'] !=-1:\n",
    "            plt.plot(row['Longitude'],row['Latitude'], marker='o', color=cmap(int(row['color'])), markersize=2)\n",
    "        else:\n",
    "                plt.plot(row['Longitude'],row['Latitude'], marker='x', color='black', markersize=5)\n",
    "        \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11769831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The visulisation requires a long time, please be patient. Or you can try new_data or any other subset of original dataset\n",
    "graphClusters_DBSCAN(data,dbscan.labels_,[])\n",
    "# the black stars are alll outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffa7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ff2e80",
   "metadata": {},
   "source": [
    "## Open questions: Can you try other eps and min_sample to improve the results of clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805bc4a6",
   "metadata": {},
   "source": [
    "The best tutorial of a Python library is always the official documentation, for example, `DBSCAN`: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86809ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1548631",
   "metadata": {},
   "source": [
    "# Recommender system with decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a280e",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e18ada",
   "metadata": {},
   "source": [
    "- A matrix factorization technique where a matrix `M` is factorized into two matrices \\(U\\) and \\(V\\), with the constraint that all three matrices have no negative elements.\n",
    "- Used to discover latent features underlying the interactions between two types of entities (e.g., users and items)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a6bb3",
   "metadata": {},
   "source": [
    "Goal:\n",
    "- Captures complex user preferences and item characteristics.\n",
    "- Helps in providing personalized recommendations.\n",
    "- Offers insights into latent factors driving user-item interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d3202",
   "metadata": {},
   "source": [
    "Therefore we can consider decomposition. \n",
    "We convert the utility matrix $M (r \\times m)$ into $M' = UV^T \\approx M$  with $U (r \\times l)$ and $V^T (l \\times m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4afb9",
   "metadata": {},
   "source": [
    "loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ratings = pd.read_csv('data/ratings.csv')\n",
    "\n",
    "# sample dataset\n",
    "# be careful, once again a very heavy operation\n",
    "ratings = ratings[:1000]\n",
    "\n",
    "print(ratings.head())\n",
    "\n",
    "# print some information\n",
    "noMovies = len(ratings['movieId'].unique())\n",
    "noUsers = len(ratings['userId'].unique())\n",
    "print(str(noMovies)+\" from \"+str(noUsers)+' users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11a9a0",
   "metadata": {},
   "source": [
    "Do some pre-processing before we carefully look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3bf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty utility matrix\n",
    "utility = np.zeros(shape=(noUsers,noMovies))\n",
    "\n",
    "# store movieIds as indices to use in utility matrix\n",
    "movieIds = {}\n",
    "midi = 0\n",
    "for value in ratings['movieId'].unique():\n",
    "    movieIds[value]=midi\n",
    "    midi = midi + 1\n",
    "\n",
    "# populate utility matrix\n",
    "for index, line in ratings.iterrows():\n",
    "    uid = int(line['userId'])-1\n",
    "    mid = movieIds[line['movieId']]\n",
    "    rating = line['rating']\n",
    "    utility[uid,mid]=rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c2b3a8",
   "metadata": {},
   "source": [
    "- utility: The original user-item unitily matrix.\n",
    "- U: The user feature matrix.\n",
    "- V: The item feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a2f28a",
   "metadata": {},
   "source": [
    "Now let's do the matrix factorisation with `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936af51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = NMF(n_components=50, init='random', random_state=0, max_iter=500)\n",
    "U = decomposition.fit_transform(utility)\n",
    "V_T = decomposition.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb9fa5",
   "metadata": {},
   "source": [
    "In the matrix `U`: \n",
    "- Each row represents a user.\n",
    "- Each column represents a latent feature.\n",
    "- Elements of \\(U\\) indicate the association strength between users and latent features.\n",
    "- In our case, latent features might include genres or themes like action, romance, or sci-fi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f7a0d",
   "metadata": {},
   "source": [
    "In the matrix `V`\n",
    "- Each row represents a latent feature.\n",
    "- Each column represents an item.\n",
    "- Elements of \\(V\\) indicate how strongly an item is associated with each latent feature.\n",
    "- This helps in understanding what characteristics of items appeal to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of U (#reviewers x #latent factors): ', np.shape(U))\n",
    "print('Shape of V_T (#latent factors x #movies): ', np.shape(V_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22325f42",
   "metadata": {},
   "source": [
    "Now we can calculate $M'$ and compare how close `M'` and `M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ = np.dot(U, V_T)\n",
    "print(np.shape(M_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab7f3d",
   "metadata": {},
   "source": [
    "We can see how more dimensions provide a closer approximation of the original matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08acf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be patient, this code is very slow (how slow depending on the max_iter and n_comp).\n",
    "# You can try some small values on range() or max_iter\n",
    "for n_comp in range(20,100,10):\n",
    "    decomposition = NMF(n_components=n_comp, init='random', random_state=1, max_iter=200)\n",
    "    U = decomposition.fit_transform(utility)\n",
    "    V_T = decomposition.components_\n",
    "    M_ = np.dot(U, V_T)\n",
    "    \n",
    "    # calculate difference between both matrices\n",
    "    diff = utility-M_\n",
    "    print(f\"for {n_comp} components,\\tdifference was {np.sum(diff)}\")\n",
    "    \n",
    "print(\"Completed!\") # this will run for a while, up to a minute or two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa726e",
   "metadata": {},
   "source": [
    "more reading here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "What do you see? is the difference between matrixes getting larger or smaller? is that good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecc24e",
   "metadata": {},
   "source": [
    "## Open Questions: Can you store all `diff` and plot a figure, the number of components vs diff, and observe the pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83acbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
